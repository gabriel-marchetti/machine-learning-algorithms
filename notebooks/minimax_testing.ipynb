{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e4db537",
   "metadata": {},
   "source": [
    "# Testing aspects of Minimax Group Fairness\n",
    "URL: https://arxiv.org/abs/2011.03108"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa19d554",
   "metadata": {},
   "source": [
    "Try simulate \"Two-Player Game Formulation\" proposed by the article to see how i can fit it under the package Temis.\n",
    "\n",
    "GENERAL DESCRIPTION:\n",
    "\n",
    "Regulator: Tries to identify which group has great loss and increase it's weight through exponential weights.\n",
    "\n",
    "Learner: Minimize current model and seek for optimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d338dad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9890b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimaxFairness:\n",
    "    def __init__(self, model_class, iterations=100, lr=0.5, verbose=False):\n",
    "        self.model_class = model_class\n",
    "        self.T = iterations\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Initialize storage for models, lambdas, and group losses history\n",
    "        self.models = []\n",
    "        self.lambdas_history = []\n",
    "        self.group_losses_history = []\n",
    "    def fit(self, X, y, groups, verbose=False):\n",
    "        n_samples = len(y)\n",
    "\n",
    "        # I don't think this way will work for intersection of groups.\n",
    "        unique_groups = np.unique(groups)\n",
    "        n_groups = len(unique_groups)\n",
    "\n",
    "        group_counts = {g: np.sum(groups == g) for g in unique_groups}\n",
    "        self.lambdas = {g: group_counts[g] / n_samples for g in unique_groups}\n",
    "\n",
    "        if verbose == True:\n",
    "            print(f\"Iniciando Jogo Minimax com {self.T} rodadas...\")\n",
    "\n",
    "        for t in range(1, self.T + 1):\n",
    "            sample_weights = np.ones(n_samples)\n",
    "\n",
    "            h_t = self.model_class(solver='lbfgs', max_iter=100)\n",
    "            h_t.fit(X, y, sample_weight=sample_weights)\n",
    "            self.models.append(h_t)\n",
    "\n",
    "            group_losses = {}\n",
    "            probs = h_t.predict_proba(X)\n",
    "\n",
    "            for g in unique_groups:\n",
    "                mask = (groups == g)\n",
    "                loss_k = log_loss(y[mask], probs[mask])\n",
    "                group_losses[g] = loss_k\n",
    "\n",
    "            self.group_losses_history.append(group_losses)\n",
    "            self.lambdas_history.append(self.lambdas.copy())\n",
    "\n",
    "            for g in unique_groups:\n",
    "                self.lambdas[g] *= np.exp(self.lr * group_losses[g])\n",
    "    def predict_proba(self, X):\n",
    "        all_probs = np.array([model.predict_proba(X)] for model in self.models)\n",
    "        return np.mean(all_probs, axis=0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "\n",
    "    def debug_step(self, t):\n",
    "        idx = t - 1\n",
    "        if idx >= len(self.lambdas_history):\n",
    "            print('Debug: Histórico ainda não disponível para essa iteração.')\n",
    "            return\n",
    "\n",
    "        current_lambdas = self.lambdas_history[idx]\n",
    "        current_group_losses = self.group_losses_history[idx]\n",
    "        groups = sorted(current_lambdas.keys())\n",
    "\n",
    "        print(f\"\\n----- [DEBUG] Rodada {t} -----\")\n",
    "        print(\"1. Jogada do Regulador (Pesos definidos):\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4fb60a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.10394707 -0.02122631  0.02497732  0.032889    0.19497393  0.06218199\n",
      "  -0.00995388 -0.22468309  0.00296418 -0.11127511  0.12740759 -0.086235\n",
      "  -0.01550448 -0.17103389 -0.07877878 -0.01402433 -0.08433013 -0.05990881\n",
      "   0.04956577  0.0870589 ]]\n",
      "--- Baseline (Standard Logistic Regression) ---\n",
      "Log Loss Grupo 0 (Fácil): 0.4144\n",
      "Log Loss Grupo 1 (Difícil): 0.8712\n",
      "Diferença de Erro: 0.4568\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 0 is out of bounds for array of dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAxisError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m mm_model = MinimaxFairness(LogisticRegression, iterations=\u001b[32m10\u001b[39m, lr=\u001b[32m0.5\u001b[39m)\n\u001b[32m     28\u001b[39m mm_model.fit(X, y, groups)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m mm_pred_probs = \u001b[43mmm_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m mm_loss_class_0 = log_loss(y[groups == \u001b[32m0\u001b[39m], mm_pred_probs[groups == \u001b[32m0\u001b[39m])\n\u001b[32m     32\u001b[39m mm_loss_class_1 = log_loss(y[groups == \u001b[32m1\u001b[39m], mm_pred_probs[groups == \u001b[32m1\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mMinimaxFairness.predict_proba\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict_proba\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m     42\u001b[39m     all_probs = np.array([model.predict_proba(X)] \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.models)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gabriel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3860\u001b[39m, in \u001b[36mmean\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, where)\u001b[39m\n\u001b[32m   3857\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3858\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis=axis, dtype=dtype, out=out, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m3860\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_methods\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3861\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gabriel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\_core\\_methods.py:123\u001b[39m, in \u001b[36m_mean\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, where)\u001b[39m\n\u001b[32m    119\u001b[39m arr = asanyarray(a)\n\u001b[32m    121\u001b[39m is_float16_result = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m rcount = \u001b[43m_count_reduce_items\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m rcount == \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m where \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m umr_any(rcount == \u001b[32m0\u001b[39m, axis=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    125\u001b[39m     warnings.warn(\u001b[33m\"\u001b[39m\u001b[33mMean of empty slice.\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gabriel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\_core\\_methods.py:86\u001b[39m, in \u001b[36m_count_reduce_items\u001b[39m\u001b[34m(arr, axis, keepdims, where)\u001b[39m\n\u001b[32m     84\u001b[39m     items = \u001b[32m1\u001b[39m\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m axis:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m         items *= arr.shape[mu.normalize_axis_index(ax, arr.ndim)]\n\u001b[32m     87\u001b[39m     items = nt.intp(items)\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     89\u001b[39m     \u001b[38;5;66;03m# TODO: Optimize case when `where` is broadcast along a non-reduction\u001b[39;00m\n\u001b[32m     90\u001b[39m     \u001b[38;5;66;03m# axis and full sum is more excessive than needed.\u001b[39;00m\n\u001b[32m     91\u001b[39m \n\u001b[32m     92\u001b[39m     \u001b[38;5;66;03m# guarded to protect circular imports\u001b[39;00m\n",
      "\u001b[31mAxisError\u001b[39m: axis 0 is out of bounds for array of dimension 0"
     ]
    }
   ],
   "source": [
    "random_state = 42\n",
    "\n",
    "np.random.seed(random_state)\n",
    "n_samples = 1000\n",
    "X, y = make_classification(n_samples=n_samples, n_features=20, n_informative=10, n_redundant=10, random_state=random_state)\n",
    "groups = np.random.choice([0, 1], size=n_samples, p=[0.7, 0.3])\n",
    "\n",
    "# Add noise to some group to make it harder to guess.\n",
    "noise_idxs = np.where(groups == 1)[0]\n",
    "y[noise_idxs] = np.random.choice([0, 1], size=len(noise_idxs))\n",
    "\n",
    "# Add some baseline model and test error rates.\n",
    "baseline_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "baseline_model.fit(X, y)\n",
    "baseline_preds = baseline_model.predict(X)\n",
    "print(baseline_model.coef_)\n",
    "\n",
    "baseline_loss_class_0 = log_loss(y[groups == 0], baseline_model.predict_proba(X)[groups == 0])\n",
    "baseline_loss_class_1 = log_loss(y[groups == 1], baseline_model.predict_proba(X)[groups == 1])\n",
    "\n",
    "print(\"--- Baseline (Standard Logistic Regression) ---\")\n",
    "print(f\"Log Loss Grupo 0 (Fácil): {baseline_loss_class_0:.4f}\")\n",
    "print(f\"Log Loss Grupo 1 (Difícil): {baseline_loss_class_1:.4f}\")\n",
    "print(f\"Diferença de Erro: {abs(baseline_loss_class_0 - baseline_loss_class_1):.4f}\")\n",
    "\n",
    "# Add Minimax Fairness model and test error rates.\n",
    "mm_model = MinimaxFairness(LogisticRegression, iterations=10, lr=0.5)\n",
    "mm_model.fit(X, y, groups)\n",
    "\n",
    "mm_pred_probs = mm_model.predict_proba(X)\n",
    "mm_loss_class_0 = log_loss(y[groups == 0], mm_pred_probs[groups == 0])\n",
    "mm_loss_class_1 = log_loss(y[groups == 1], mm_pred_probs[groups == 1])\n",
    "\n",
    "print(\"\\n--- Minimax Fair Model (Após 100 iterações) ---\")\n",
    "print(f\"Log Loss Grupo 0 (Fácil): {mm_loss_class_0:.4f}\")\n",
    "print(f\"Log Loss Grupo 1 (Difícil): {mm_loss_class_1:.4f}\")\n",
    "print(f\"Diferença de Erro: {abs(mm_loss_class_0 - mm_loss_class_1):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e8d2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c2953f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
